{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5c0f92-7335-4446-8f15-69c8d6ba691c",
   "metadata": {},
   "source": [
    "Assignment #1\n",
    "\n",
    "Create or find your own corpus. This corpus should have the following characteristics:\n",
    "• Three different subcorpora, belonging to three different genres (aka registers or text types)\n",
    "• Each genre at least 5,000 words long\n",
    "\n",
    "By genre I mean a type of text that differs from other types of text in content, audience, or mode of delivery. A newspaper article and a podcast about electric vehicles may have similar content but have different audiences and modes of delivery. A lecture and an exam may have similar content and audience, but a different mode of delivery (one oral and one written). \n",
    "\n",
    "You have many options. You can collect your own email or text messages from several days, your own papers or other course work, or you can collect from external sources. \n",
    "A few things to take into account:\n",
    "• If you use email or text messages, you can only include text sent by you. Text by other participants (including threads in quotes in your own messages) needs to be excluded for confidentiality reasons. Alternatively, you can ask the people involved to give you permission to use the data for research purposes. This also includes text posted on social networks, where a password or permission is required to access the data. \n",
    "• If you use text from web sites, you do not normally need permission, unless the site is protected under a password.\n",
    "• Newspaper text is certainly the easiest to find, but please make sure you have distinct genres within it. For instance, you could use a high-brow vs. low-brow newspaper, or opinion pieces vs. news articles.\n",
    "• You will need to save the corpora as plain text.\n",
    "• It is fine to use all written text, but if you have access to transcribed sources of spoken language, feel free to use that as well.\n",
    "• Please include a reference to the source of each genre in your assignment.\n",
    "\n",
    "Here is what you need to submit, for each subcorpus:\n",
    "1. The length (in words).\n",
    "   Command: len(text)\n",
    "3. The lexical diversity.\n",
    "   Command: lexical_diversity(text)\n",
    "5. Top 10 most frequent words and their counts.\n",
    "   Command:\n",
    "   fdist1 = FreqDist(text)\n",
    "   fdist1.most_common(10)\n",
    "7. Words that are at least 10 characters long and their counts.\n",
    "8. The longest sentence (type the sentence and give the number of words). Hint: look at the Gutenberg part of Section 2.1 in NLTK.\n",
    "9. A stemmed version of the longest sentence.\n",
    "10. Overall (not for each subcorpus): A reflection (1 paragraph or so): What do the most frequent words, the longest words, and longest sentence tell you about each of the 3 genres? How do you interpret the lexical diversity?\n",
    "\n",
    "For the assignment, you need to submit on Canvas:\n",
    "• The answer to the six questions, for each subcorpus, and the answer to question 7. Please submit this as a pdf, with clear indication of which corpus the answer is for.\n",
    "• The commands used to arrive at the answer (a pdf of your notebook or a link to a GitHub repository). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e600c1f-9615-4767-ac3f-a6e14e373310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664f23ef-3afc-4ce0-911a-ee1b2146cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e2a0a7-96cd-4a0c-a4a0-82cdebba517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/catalinaporime/Desktop/SDA250/Assignment_1/Dahmer.txt\", \"r\", encoding = \"utf8\") as f:\n",
    "    Dahmer = f.read()\n",
    "    \n",
    "DahmerTokens = nltk.word_tokenize(Dahmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5eaaaf47-8f01-43ef-95cb-815ffa7319b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Dahmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "50fd229a-e284-43a0-af17-090e59938fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question #1\n",
      "8084\n",
      "The number of words in the Jeffrey Dahmer documentary on YouTube: 8084\n"
     ]
    }
   ],
   "source": [
    "#Q1:\n",
    "print(\"Question #1\")\n",
    "lengthDahmer = len(DahmerTokens)\n",
    "print(lengthDahmer)\n",
    "print(\"The number of words in the Jeffrey Dahmer documentary on YouTube:\", lengthDahmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6a34aaec-a8af-4099-b1ba-d2a843367fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question #2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19594260267194458"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2:\n",
    "print(\"Question #2\")\n",
    "lexical_diversity(DahmerTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aa087fa6-9d14-479f-940b-1eecdab38052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question #3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 420),\n",
       " ('.', 388),\n",
       " ('the', 301),\n",
       " ('of', 199),\n",
       " ('to', 194),\n",
       " ('was', 189),\n",
       " ('he', 171),\n",
       " ('a', 165),\n",
       " ('and', 144),\n",
       " ('that', 134),\n",
       " ('in', 132),\n",
       " ('-', 127),\n",
       " ('Dahmer', 115)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3:\n",
    "print(\"Question #3\")\n",
    "from nltk.probability import FreqDist\n",
    "fdist1 = FreqDist(DahmerTokens)\n",
    "fdist1.most_common(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a6c0e5b7-b53e-466d-a49e-3aa687d2a5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question #4\n",
      "[('television', 6), ('themselves', 5), ('confession', 5), ('necrophilia', 4), ('detectives', 4), ('cannibalism', 3), ('constantly', 3), ('completely', 3), ('interested', 3), ('remembered', 3), ('grandmother', 3), ('14-year-old', 3), ('photographs', 3), ('essentially', 3), ('immediately', 3), ('one-bedroom', 2), ('investigate', 2), ('31-year-old', 2), ('strangling', 2), ('psychological', 2), ('classmates', 2), ('collection', 2), ('apparently', 2), ('neighborhood', 2), ('hitchhiker', 2), ('ultimately', 2), ('eventually', 2), ('dismembered', 2), ('homosexual', 2), ('opportunity', 2), ('predominantly', 2), ('atrocities', 2), ('13-year-old', 2), ('unconscious', 2), ('necrophiliac', 2), ('refrigerator', 2), ('potentially', 2), ('circumstances', 2), ('incredible', 2), ('controlling', 2), ('assistants', 2), ('whispering', 1), ('synonymous', 1), ('revelations', 1), ('journalist', 1), ('antiseptic', 1), ('throughout', 1), ('playground', 1), ('ostracized', 1), ('surrounding', 1), ('progressed', 1), ('fascinated', 1), ('decapitating', 1), ('disturbing', 1), ('intoxicated', 1), ('experiences', 1), ('ruminating', 1), ('fantasizing', 1), ('19-year-old', 1), ('abandonment', 1), ('pulverized', 1), ('sledgehammer', 1), ('experienced', 1), ('accustomed', 1), ('apprehended', 1), ('relentless', 1), ('Culpability', 1), ('compromise', 1), ('desecrating', 1), ('University', 1), ('colleagues', 1), ('dependency', 1), ('21-year-old', 1), ('discharged', 1), ('approaching', 1), ('frequenting', 1), ('bathhouses', 1), ('stigmatized', 1), ('underground', 1), ('companionship', 1), ('conversations', 1), ('prospective', 1), ('supportive', 1), ('presumably', 1), ('Dissolving', 1), ('identifying', 1), ('committing', 1), ('complaining', 1), ('benzodiazepines', 1), ('sentencing', 1), ('unrelenting', 1), ('unbeknownst', 1), ('authorities', 1), ('29-year-old', 1), ('dismembering', 1), ('constructing', 1), ('diabolical', 1), ('performing', 1), ('sacrificial', 1), ('approached', 1), ('cannibalize', 1), ('metaphorical', 1), ('incorporate', 1), ('experimenting', 1), ('semi-conscious', 1), ('subservient', 1), ('attempting', 1), ('incredibly', 1), ('unpleasant', 1), ('personality', 1), ('manufacturing', 1), ('Midwestern', 1), ('collecting', 1), ('cannibalizing', 1), ('successful', 1), ('African-American', 1), ('infrequently', 1), ('agonizingly', 1), ('Sinthasomphone', 1), ('attractive', 1), ('remarkable', 1), ('characteristic', 1), ('psychopaths', 1), ('extraordinarily', 1), ('high-pressure', 1), ('decomposing', 1), ('minorities', 1), ('continuous', 1), ('fulfillment', 1), ('insatiable', 1), ('dismemberment', 1), ('speculation', 1), ('struggling', 1), ('officially', 1), ('resistance', 1), ('investigation', 1), ('unbelievable', 1), ('fantastical', 1), ('international', 1), ('incredulous', 1), ('everything', 1), ('confessions', 1), ('involvement', 1), ('interesting', 1), ('differentiate', 1), ('investigators', 1), ('preliminary', 1), ('appearance', 1), ('good-looking', 1), ('responsible', 1), ('psychiatry', 1), ('definition', 1), ('paraphilias', 1), ('implications', 1), ('consultant', 1), ('necrophiliacs', 1), ('accomplish', 1), ('necrophilous', 1), ('specialist', 1), ('psychiatrists', 1), ('psychiatric', 1), ('traditional', 1), ('imprisonment', 1), ('especially', 1), ('hoodwinked', 1), ('Correctional', 1), ('disconcerting', 1), ('respectful', 1), ('forgiveness', 1), ('Christopher', 1), ('bludgeoned', 1), ('demolished', 1), ('impossible', 1), ('Ultimately', 1), ('inexplicably', 1), ('absolutely', 1), ('perversions', 1), ('nightmares', 1), ('unquestionably', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Q4:\n",
    "print(\"Question #4\")\n",
    "dahmerLong = [word for word in DahmerTokens if len(word) >= 10]\n",
    "longDist = FreqDist(dahmerLong)\n",
    "print(longDist.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "aca2792b-2c16-449b-9333-147b549ea1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question #5\n",
      "Longest sentence: - And then when the guy said he wanted to leave, Dahmer clubbed him on the back of the head with a barbell\n",
      "and then strangled him, then ultimately disposed of the body,\n",
      "removed all the flesh, and eventually dissolved it in acid\n",
      "and pulverized the bones with a sledgehammer.\n",
      "Number of words: 55\n"
     ]
    }
   ],
   "source": [
    "#Q5:\n",
    "print(\"Question #5\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentences = sent_tokenize(Dahmer)\n",
    "longest_sentence = max(sentences, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "word_count = len(word_tokenize(longest_sentence))\n",
    "print(\"Longest sentence:\", longest_sentence)\n",
    "print(\"Number of words:\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2665dfab-a692-442c-a9dc-417c97b97989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question #6\n",
      "['-', 'and', 'then', 'when', 'the', 'guy', 'said', 'he', 'want', 'to', 'leav', ',', 'dahmer', 'club', 'him', 'on', 'the', 'back', 'of', 'the', 'head', 'with', 'a', 'barbel', 'and', 'then', 'strangl', 'him', ',', 'then', 'ultim', 'dispos', 'of', 'the', 'bodi', ',', 'remov', 'all', 'the', 'flesh', ',', 'and', 'eventu', 'dissolv', 'it', 'in', 'acid', 'and', 'pulver', 'the', 'bone', 'with', 'a', 'sledgehamm', '.']\n"
     ]
    }
   ],
   "source": [
    "#Q6:\n",
    "print(\"Question #6\")\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer (\"english\")\n",
    "long1 = word_tokenize(longest_sentence)\n",
    "stemmed_word = [stemmer.stem(word) for word in long1]\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853a06d-b748-42ab-b985-e06088938ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
