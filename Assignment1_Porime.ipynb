{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5c0f92-7335-4446-8f15-69c8d6ba691c",
   "metadata": {},
   "source": [
    "Assignment #1 - Sub Corpora 2 \n",
    "(Transcript of a YouTube documentary on Jeffery Dahmer - https://www.youtube.com/watch?v=Y1EWmrzD2Mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a853a06d-b748-42ab-b985-e06088938ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question #1\n",
      "8084\n",
      "The number of words in the Jeffrey Dahmer documentary on YouTube: 8084\n",
      "\n",
      "Question #2\n",
      "0.19594260267194458\n",
      "The lexical diversity of the text is: 0.19594260267194458\n",
      "\n",
      "Question #3\n",
      "These are the 10 most common words in the text and their count:\n",
      "[(',', 420), ('.', 388), ('the', 301), ('of', 199), ('to', 194), ('was', 189), ('he', 171), ('a', 165), ('and', 144), ('that', 134), ('in', 132), ('-', 127), ('Dahmer', 115)]\n",
      "\n",
      "Question #4\n",
      "These are the words that are at least 10 characters long and their count: [('television', 6), ('themselves', 5), ('confession', 5), ('necrophilia', 4), ('detectives', 4), ('cannibalism', 3), ('constantly', 3), ('completely', 3), ('interested', 3), ('remembered', 3), ('grandmother', 3), ('14-year-old', 3), ('photographs', 3), ('essentially', 3), ('immediately', 3), ('one-bedroom', 2), ('investigate', 2), ('31-year-old', 2), ('strangling', 2), ('psychological', 2), ('classmates', 2), ('collection', 2), ('apparently', 2), ('neighborhood', 2), ('hitchhiker', 2), ('ultimately', 2), ('eventually', 2), ('dismembered', 2), ('homosexual', 2), ('opportunity', 2), ('predominantly', 2), ('atrocities', 2), ('13-year-old', 2), ('unconscious', 2), ('necrophiliac', 2), ('refrigerator', 2), ('potentially', 2), ('circumstances', 2), ('incredible', 2), ('controlling', 2), ('assistants', 2), ('whispering', 1), ('synonymous', 1), ('revelations', 1), ('journalist', 1), ('antiseptic', 1), ('throughout', 1), ('playground', 1), ('ostracized', 1), ('surrounding', 1), ('progressed', 1), ('fascinated', 1), ('decapitating', 1), ('disturbing', 1), ('intoxicated', 1), ('experiences', 1), ('ruminating', 1), ('fantasizing', 1), ('19-year-old', 1), ('abandonment', 1), ('pulverized', 1), ('sledgehammer', 1), ('experienced', 1), ('accustomed', 1), ('apprehended', 1), ('relentless', 1), ('Culpability', 1), ('compromise', 1), ('desecrating', 1), ('University', 1), ('colleagues', 1), ('dependency', 1), ('21-year-old', 1), ('discharged', 1), ('approaching', 1), ('frequenting', 1), ('bathhouses', 1), ('stigmatized', 1), ('underground', 1), ('companionship', 1), ('conversations', 1), ('prospective', 1), ('supportive', 1), ('presumably', 1), ('Dissolving', 1), ('identifying', 1), ('committing', 1), ('complaining', 1), ('benzodiazepines', 1), ('sentencing', 1), ('unrelenting', 1), ('unbeknownst', 1), ('authorities', 1), ('29-year-old', 1), ('dismembering', 1), ('constructing', 1), ('diabolical', 1), ('performing', 1), ('sacrificial', 1), ('approached', 1), ('cannibalize', 1), ('metaphorical', 1), ('incorporate', 1), ('experimenting', 1), ('semi-conscious', 1), ('subservient', 1), ('attempting', 1), ('incredibly', 1), ('unpleasant', 1), ('personality', 1), ('manufacturing', 1), ('Midwestern', 1), ('collecting', 1), ('cannibalizing', 1), ('successful', 1), ('African-American', 1), ('infrequently', 1), ('agonizingly', 1), ('Sinthasomphone', 1), ('attractive', 1), ('remarkable', 1), ('characteristic', 1), ('psychopaths', 1), ('extraordinarily', 1), ('high-pressure', 1), ('decomposing', 1), ('minorities', 1), ('continuous', 1), ('fulfillment', 1), ('insatiable', 1), ('dismemberment', 1), ('speculation', 1), ('struggling', 1), ('officially', 1), ('resistance', 1), ('investigation', 1), ('unbelievable', 1), ('fantastical', 1), ('international', 1), ('incredulous', 1), ('everything', 1), ('confessions', 1), ('involvement', 1), ('interesting', 1), ('differentiate', 1), ('investigators', 1), ('preliminary', 1), ('appearance', 1), ('good-looking', 1), ('responsible', 1), ('psychiatry', 1), ('definition', 1), ('paraphilias', 1), ('implications', 1), ('consultant', 1), ('necrophiliacs', 1), ('accomplish', 1), ('necrophilous', 1), ('specialist', 1), ('psychiatrists', 1), ('psychiatric', 1), ('traditional', 1), ('imprisonment', 1), ('especially', 1), ('hoodwinked', 1), ('Correctional', 1), ('disconcerting', 1), ('respectful', 1), ('forgiveness', 1), ('Christopher', 1), ('bludgeoned', 1), ('demolished', 1), ('impossible', 1), ('Ultimately', 1), ('inexplicably', 1), ('absolutely', 1), ('perversions', 1), ('nightmares', 1), ('unquestionably', 1)]\n",
      "\n",
      "Question #5\n",
      "Longest sentence: - And then when the guy said he wanted to leave, Dahmer clubbed him on the back of the head with a barbell\n",
      "and then strangled him, then ultimately disposed of the body,\n",
      "removed all the flesh, and eventually dissolved it in acid\n",
      "and pulverized the bones with a sledgehammer.\n",
      "Number of words: 55\n",
      "\n",
      "Question #6\n",
      "This is the stemmed version of the longest sentence:\n",
      "['-', 'and', 'then', 'when', 'the', 'guy', 'said', 'he', 'want', 'to', 'leav', ',', 'dahmer', 'club', 'him', 'on', 'the', 'back', 'of', 'the', 'head', 'with', 'a', 'barbel', 'and', 'then', 'strangl', 'him', ',', 'then', 'ultim', 'dispos', 'of', 'the', 'bodi', ',', 'remov', 'all', 'the', 'flesh', ',', 'and', 'eventu', 'dissolv', 'it', 'in', 'acid', 'and', 'pulver', 'the', 'bone', 'with', 'a', 'sledgehamm', '.']\n"
     ]
    }
   ],
   "source": [
    "#loading packages \n",
    "import nltk\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "#loading data\n",
    "with open(\"/Users/catalinaporime/Desktop/SDA250/Assignment_1/Dahmer.txt\", \"r\", encoding = \"utf8\") as f:\n",
    "    Dahmer = f.read()\n",
    "\n",
    "#tokenizing the data   \n",
    "DahmerTokens = nltk.word_tokenize(Dahmer)\n",
    "\n",
    "#printing text (commented due to length)\n",
    "#print(Dahmer)\n",
    "\n",
    "\n",
    "#Q1:\n",
    "print(\"Question #1\")\n",
    "lengthDahmer = len(DahmerTokens)\n",
    "print(lengthDahmer)\n",
    "print(\"The number of words in the Jeffrey Dahmer documentary on YouTube:\", lengthDahmer)\n",
    "print()\n",
    "\n",
    "#Q2:\n",
    "print(\"Question #2\")\n",
    "lex_div = lexical_diversity(DahmerTokens)\n",
    "print(lex_div)\n",
    "print(\"The lexical diversity of the text is:\", lex_div)\n",
    "print()\n",
    "\n",
    "#Q3:\n",
    "print(\"Question #3\")\n",
    "from nltk.probability import FreqDist\n",
    "fdist1 = FreqDist(DahmerTokens)\n",
    "fdist1.most_common(13)\n",
    "print(\"These are the 10 most common words in the text and their count:\")\n",
    "print(fdist1.most_common(13))\n",
    "print()\n",
    "\n",
    "#Q4:\n",
    "print(\"Question #4\")\n",
    "dahmerLong = [word for word in DahmerTokens if len(word) >= 10]\n",
    "longDist = FreqDist(dahmerLong)\n",
    "print(\"These are the words that are at least 10 characters long and their count:\", longDist.most_common())\n",
    "print()\n",
    "\n",
    "#Q5:\n",
    "print(\"Question #5\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentences = sent_tokenize(Dahmer)\n",
    "longest_sentence = max(sentences, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "word_count = len(word_tokenize(longest_sentence))\n",
    "print(\"Longest sentence:\", longest_sentence)\n",
    "print(\"Number of words:\", word_count)\n",
    "print()\n",
    "\n",
    "#Q6:\n",
    "print(\"Question #6\")\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer (\"english\")\n",
    "long1 = word_tokenize(longest_sentence)\n",
    "stemmed_word = [stemmer.stem(word) for word in long1]\n",
    "print(\"This is the stemmed version of the longest sentence:\")\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3bd8e0-8849-4028-8f1c-31ca61ba0223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
